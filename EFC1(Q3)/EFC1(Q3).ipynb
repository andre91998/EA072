{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EFC1 (Q3): Multi-Layer Perceptron, Keras Framework, Tensorflow Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we began with a basic MLP network, with one hidden layer with 500 neurons and 0.5 dropout. The code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for mean loss and accuracy, see initial.txt\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "AccuracySum=0\n",
    "bestAccuracy=[1,0,0,0,0.2]\n",
    "\n",
    "print(\"\\nepoch: 5 ; neurons: 512 ; dropout: 0.5 \\n\")\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "tf.keras.layers.Flatten(),\n",
    "tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "tf.keras.layers.Dropout(0.5),\n",
    "tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "loss='sparse_categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "evaluation = model.evaluate(x_test, y_test)\n",
    "model_json = model.to_json()\n",
    "\n",
    "bestAccuracy = [1, 5, 512, 0.5, evaluation[0], evaluation[1]]\n",
    "\n",
    "json_file = open(\"model_MLP.json\", \"w\")\n",
    "json_file.write(model_json)\n",
    "json_file.close()\n",
    "model.save_weights(\"model_MLP.h5\")\n",
    "print(\"Model saved to disk\")\n",
    "os.getcwd()\n",
    "\n",
    "f=open(\"initial.txt\",\"a+\")\n",
    "f.write(str(bestAccuracy[0])+\" ; \"+str(bestAccuracy[1])+\" ; \"+str(bestAccuracy[2])+\" ; \"+str(bestAccuracy[3])+\" ; \"+str(bestAccuracy[4])+\" ; \"+str(bestAccuracy[5])+\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the network 4 times to calculate and average loss and average accuracy. We obtained:\n",
    "\n",
    "Average Loss: 0.067664\n",
    "Average Accuracy: 0.979025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we alter the original MLP to increase accuracy. To do so, we adopt a \"trial and error\" approach, altering the key parameters of the network with a for loop. We experimented with: number of hidden layers, epochs, number of neurons and dropout. As to not occupy all the RAM at once, and avoid using SWAP memory, one code was created for each situation: one, two, and three hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\"\n",
    "%-----------------------------------------------------------------------------%\n",
    "%Author: André Barros de Medeiros\n",
    "%Date:09/14/2019\n",
    "%Copyright: free to use, copy, and modify\n",
    "%Description: Multi-Layer Perceptron to classify MNIST dataset images\n",
    "%Important: Layers: 2 (600 neurons, 10 neurons)\n",
    "%           Epochs: 6\n",
    "%           Dropout: 0.4 (first layer)\n",
    "%           Activation Function: RELU (first layer)\n",
    "%           Optimizer Algorithm: ADAM\n",
    "%           Loss Function: Cross Entropy\n",
    "%\n",
    "%           Accuracy: 0.9819, gain of 0.008\n",
    "%-----------------------------------------------------------------------------%\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "    \n",
    "AccuracySum=0\n",
    "bestAccuracy=[1,0,0,0,0.2]\n",
    "\n",
    "for epoch in [7,8]:\n",
    "    for neurons in [300,512]:\n",
    "        for dropout in [0.2,0.3,0.4, 0.5]:\n",
    "#epoch = 2\n",
    "#neurons =500\n",
    "#dropout=0.5\n",
    "            \n",
    "            print(\"\\nepoch: \" + str(epoch)+\"; neurons: \"+str(neurons)+\"; dropout: \"+str(dropout) + \"\\n\")\n",
    "            \n",
    "            (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "            x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "            \n",
    "            model = tf.keras.models.Sequential([\n",
    "             tf.keras.layers.Flatten(),\n",
    "             tf.keras.layers.Dense(neurons, activation=tf.nn.relu),\n",
    "             tf.keras.layers.Dropout(dropout),\n",
    "             tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "            ])\n",
    "            \n",
    "            model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            model.fit(x_train, y_train, epochs=epoch)\n",
    "            evaluation = model.evaluate(x_test, y_test) #store loss and accuracy\n",
    "            \n",
    "            print( \"\\nAcurracy with: \" + str(epoch) + \" , \" + str(neurons) + \" , \" + str(dropout) + \" is \" + str(evaluation[1])+\"\\n\")\n",
    "            if evaluation[1] > bestAccuracy[4]:\n",
    "                bestAccuracy = [1, epoch, neurons, dropout, evaluation[1]]\n",
    "                print(\"\\n New Best Accuracy \\n\")\n",
    "            model_json = model.to_json()\n",
    "            \n",
    "            json_file = open(\"model_MLP.json\", \"w\")\n",
    "            json_file.write(model_json)\n",
    "            json_file.close()\n",
    "            \n",
    "            model.save_weights(\"model_MLP.h5\")\n",
    "            print(\"Model saved to disk\")\n",
    "            os.getcwd()\n",
    "\n",
    "f=open(\"one_hidden_layer.txt\",\"w+\")\n",
    "f.write(str(bestAccuracy[0])+\" ; \"+str(bestAccuracy[1])+\" ; \"+str(bestAccuracy[2])+\" ; \"+str(bestAccuracy[3])+\" ; \"+str(bestAccuracy[4]));\n",
    "f.close()            \n",
    "#print(bestAccuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\"\n",
    "%-----------------------------------------------------------------------------%\n",
    "%Author: André Barros de Medeiros\n",
    "%Date:09/14/2019\n",
    "%Copyright: free to use, copy, and modify\n",
    "%Description: Multi-Layer Perceptron to classify MNIST dataset images\n",
    "%Important: Layers: 3 (512 neurons, 512, 10 neurons)\n",
    "%           Epochs: 5\n",
    "%           Dropout: 0.5 (first layer)\n",
    "%           Activation Function: RELU (first layer)\n",
    "%           Optimizer Algorithm: ADAM\n",
    "%           Loss Function: Cross Entropy\n",
    "%\n",
    "%           Loss: 0.0665 / Accuracy: 0.9811\n",
    "%-----------------------------------------------------------------------------%\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "    \n",
    "AccuracySum=0\n",
    "bestAccuracy=[2,0,0,0,0.2]\n",
    "\n",
    "for epoch in [7,8]:\n",
    "    for neurons in [300,512]:\n",
    "        for dropout in [0.2, 0.3, 0.4]:\n",
    "        \n",
    "            print(\"\\nepoch: \" + str(epoch)+\"; neurons: \"+str(neurons)+\"; dropout: \"+str(dropout) + \"\\n\")\n",
    "            \n",
    "            (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "            x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "            \n",
    "            model = tf.keras.models.Sequential([\n",
    "             tf.keras.layers.Flatten(),\n",
    "             tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "             tf.keras.layers.Dropout(0.5),\n",
    "             tf.keras.layers.Dense(neurons, activation=tf.nn.relu),\n",
    "             tf.keras.layers.Dropout(dropout),\n",
    "             tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "            ])\n",
    "            \n",
    "            model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            model.fit(x_train, y_train, epochs=epoch)\n",
    "            evaluation = model.evaluate(x_test, y_test) #store loss and accuracy\n",
    "            \n",
    "            print( \"\\nAcurracy with: \" + str(epoch) + \" , \" + str(neurons) + \" , \" + str(dropout) + \" is \" + str(evaluation[1])+\"\\n\")\n",
    "            if evaluation[1] > bestAccuracy[4]:\n",
    "                bestAccuracy = [2, epoch, neurons, dropout, evaluation[1]]\n",
    "                print(\"\\n New Best Accuracy \\n\")\n",
    "            model_json = model.to_json()\n",
    "            \n",
    "            json_file = open(\"model_MLP.json\", \"w\")\n",
    "            json_file.write(model_json)\n",
    "            json_file.close()\n",
    "            \n",
    "            model.save_weights(\"model_MLP.h5\")\n",
    "            print(\"Model saved to disk\")\n",
    "            os.getcwd()\n",
    "\n",
    "f=open(\"two_hidden_layers.txt\",\"w+\")\n",
    "f.write(str(bestAccuracy[0])+\" ; \"+str(bestAccuracy[1])+\" ; \"+str(bestAccuracy[2])+\" ; \"+str(bestAccuracy[3])+\" ; \"+str(bestAccuracy[4]));\n",
    "f.close()            \n",
    "#print(bestAccuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\"\n",
    "%-----------------------------------------------------------------------------%\n",
    "%Author: André Barros de Medeiros\n",
    "%Date:09/14/2019\n",
    "%Copyright: free to use, copy, and modify\n",
    "%Description: Multi-Layer Perceptron to classify MNIST dataset images\n",
    "%Important: Layers: 4 (512 neurons, 512, 10 neurons)\n",
    "%           Epochs: 5\n",
    "%           Dropout: 0.5 (first layer)\n",
    "%           Activation Function: RELU (first layer)\n",
    "%           Optimizer Algorithm: ADAM\n",
    "%           Loss Function: Cross Entropy\n",
    "%\n",
    "%           Loss: 0.0665 / Accuracy: 0.9811\n",
    "%-----------------------------------------------------------------------------%\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "    \n",
    "AccuracySum=0\n",
    "bestAccuracy=[3,0,0,0,0.2]\n",
    "\n",
    "for epoch in [7,8]:\n",
    "    for neurons in [300,512]:\n",
    "        for dropout in [0.2,0.3, 0.4, 0.5]:\n",
    "        \n",
    "            print(\"\\nepoch: \" + str(epoch)+\"; neurons: \"+str(neurons)+\"; dropout: \"+str(dropout) + \"\\n\")\n",
    "            \n",
    "            (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "            x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "            \n",
    "            model = tf.keras.models.Sequential([\n",
    "             tf.keras.layers.Flatten(),\n",
    "             tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "             tf.keras.layers.Dropout(0.5),\n",
    "             tf.keras.layers.Dense(neurons, activation=tf.nn.relu),\n",
    "             tf.keras.layers.Dropout(dropout),\n",
    "             tf.keras.layers.Dense(neurons, activation=tf.nn.relu),\n",
    "             tf.keras.layers.Dropout(dropout),\n",
    "             tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "            ])\n",
    "            \n",
    "            model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            model.fit(x_train, y_train, epochs=epoch)\n",
    "            evaluation = model.evaluate(x_test, y_test) #store loss and accuracy\n",
    "            \n",
    "            print( \"\\nAcurracy with: \" + str(epoch) + \" , \" + str(neurons) + \" , \" + str(dropout) + \" is \" + str(evaluation[1])+\"\\n\")\n",
    "            if evaluation[1] > bestAccuracy[4]:\n",
    "                bestAccuracy = [2, epoch, neurons, dropout, evaluation[1]]\n",
    "                print(\"\\n New Best Accuracy \\n\")\n",
    "            model_json = model.to_json()\n",
    "            \n",
    "            json_file = open(\"model_MLP.json\", \"w\")\n",
    "            json_file.write(model_json)\n",
    "            json_file.close()\n",
    "            \n",
    "            model.save_weights(\"model_MLP.h5\")\n",
    "            print(\"Model saved to disk\")\n",
    "            os.getcwd()\n",
    "\n",
    "f=open(\"three_hidden_layers.txt\",\"w+\")\n",
    "f.write(str(bestAccuracy[0])+\" ; \"+str(bestAccuracy[1])+\" ; \"+str(bestAccuracy[2])+\" ; \"+str(bestAccuracy[3])+\" ; \"+str(bestAccuracy[4]));\n",
    "f.close()            \n",
    "#print(bestAccuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that in each code, we save the caracteristics of the network with the best accuracy. We then compare the three, and have that the best accuracy was: 0.9836 (network with 1 hidden layer, 8 epochs of training, 300 neurnos in the hidden layer, and 0.2 dropout. \n",
    "\n",
    "We now have our final network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\"\n",
    "%-----------------------------------------------------------------------------%\n",
    "%Author: André Barros de Medeiros\n",
    "%Date:09/23/2019\n",
    "%Copyright: free to use, copy, and modify\n",
    "%Description: Final Multi-Layer Perceptron to classify MNIST dataset images\n",
    "%Important: Hidden Layers: 1 (300 neurons)\n",
    "%           Epochs: 8\n",
    "%           Dropout: 0.2 (first layer)\n",
    "%           Activation Function: RELU (first layer)\n",
    "%           Optimizer Algorithm: ADAM\n",
    "%           Loss Function: Cross Entropy\n",
    "%\n",
    "% Averages (4 tests):\n",
    "%           Loss: 0.066126  / Accuracy: 0.982075\n",
    "%-----------------------------------------------------------------------------%\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "bestAccuracy=[0,0,0,0,0]\n",
    "            \n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    " tf.keras.layers.Flatten(),\n",
    " tf.keras.layers.Dense(300, activation=tf.nn.relu), #hidden layer\n",
    " tf.keras.layers.Dropout(0.2), #hidden layer dropout\n",
    " tf.keras.layers.Dense(10, activation=tf.nn.softmax) #output layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=8)\n",
    "evaluation = model.evaluate(x_test, y_test) #store loss and accuracy\n",
    "\n",
    "print( \"\\nAcurracy with: 8 epochs, 300 neurons, and 0.2 dropout is \" + str(evaluation[1])+\"\\n\")\n",
    "if evaluation[1] > bestAccuracy[4]:\n",
    "    bestAccuracy = [1, 8, 300, 0.2, evaluation[0], evaluation[1]] #[hidden layers ; epochs ; neurons in hidden layer ; hidden layer dropout, loss, accuracy]\n",
    "model_json = model.to_json()\n",
    "\n",
    "json_file = open(\"model_MLP.json\", \"w\")\n",
    "json_file.write(model_json)\n",
    "json_file.close()\n",
    "\n",
    "model.save_weights(\"model_MLP.h5\") #save weights to use \n",
    "print(\"Model saved to disk\")\n",
    "os.getcwd()\n",
    "\n",
    "f=open(\"finalMLP.txt\",\"a+\")\n",
    "f.write(str(bestAccuracy[0])+\" ; \"+str(bestAccuracy[1])+\" ; \"+str(bestAccuracy[2])+\" ; \"+str(bestAccuracy[3])+\" ; \"+str(bestAccuracy[4])+\" ; \"+str(bestAccuracy[5])+\"\\n\");\n",
    "f.close()            \n",
    "#print(bestAccuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As done before, we train the network 4 times and obtain:\n",
    "\n",
    "Average Loss: 0.066126\n",
    "Average Accuracy: 0.982075\n",
    "\n",
    "This means that we have an increase in 0.3%, which is substancial considering we are close to 100% and don't have much room to improve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
